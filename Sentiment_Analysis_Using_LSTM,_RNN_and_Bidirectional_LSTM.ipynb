{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOEioCAdtqJZkfuViIDCp2g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elixirutkarsh/Sentiment-Analysis-Using-LSTM-RNN-and-Bidirectional-LSTM/blob/main/Sentiment_Analysis_Using_LSTM%2C_RNN_and_Bidirectional_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PAd3foAji6qC"
      },
      "outputs": [],
      "source": [
        "# Utilities\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plot libraries\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "metadata": {
        "id": "hzstu8UHjLd9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"../input/imdb-movie-ratings-sentiment-analysis/movie.csv\")\n",
        "dataset.head()\n",
        "ax = dataset.groupby('label').count().plot(kind='bar', title='Distribution of data',\n",
        "                                               legend=False)\n",
        "ax = ax.set_xticklabels(['Negative','Positive'], rotation=0)\n",
        "contractions = pd.read_csv('../input/contractions/contractions.csv', index_col='Contraction')\n",
        "contractions.index = contractions.index.str.lower()\n",
        "contractions.Meaning = contractions.Meaning.str.lower()\n",
        "contractions_dict = contractions.to_dict()['Meaning']\n",
        "\n",
        "# Defining regex patterns.\n",
        "linebreaks        = \"<br /><br />\"\n",
        "alphaPattern      = \"[^a-z0-9<>]\"\n",
        "sequencePattern   = r\"(.)\\1\\1+\"\n",
        "seqReplacePattern = r\"\\1\\1\"\n",
        "\n",
        "# Defining regex for emojis\n",
        "smileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\n",
        "sademoji          = r\"[8:=;]['`\\-]?\\(+\"\n",
        "neutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n",
        "lolemoji          = r\"[8:=;]['`\\-]?p+\"\n",
        "\n",
        "def preprocess_reviews(review):\n",
        "\n",
        "    review = review.lower()\n",
        "\n",
        "    review = re.sub(linebreaks,\" \",review)\n",
        "    # Replace 3 or more consecutive letters by 2 letter.\n",
        "    review = re.sub(sequencePattern, seqReplacePattern, review)\n",
        "\n",
        "    # Replace all emojis.\n",
        "    review = re.sub(r'<3', '<heart>', review)\n",
        "    review = re.sub(smileemoji, '<smile>', review)\n",
        "    review = re.sub(sademoji, '<sadface>', review)\n",
        "    review = re.sub(neutralemoji, '<neutralface>', review)\n",
        "    review = re.sub(lolemoji, '<lolface>', review)\n",
        "\n",
        "    for contraction, replacement in contractions_dict.items():\n",
        "        review = review.replace(contraction, replacement)\n",
        "\n",
        "    # Remove non-alphanumeric and symbols\n",
        "    review = re.sub(alphaPattern, ' ', review)\n",
        "\n",
        "    # Adding space on either side of '/' to seperate words (After replacing URLS).\n",
        "    review = re.sub(r'/', ' / ', review)\n",
        "    return review\n",
        "%%time\n",
        "dataset['cleaned_review'] = dataset.text.apply(preprocess_reviews)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "dataset[\"cleaned_review\"][0]\n",
        "# j = 0\n",
        "for i in range(len(dataset)):\n",
        "    lis = []\n",
        "    for words in dataset[\"cleaned_review\"][i].split():\n",
        "        if words not in stop_words:\n",
        "            words = lemmatizer.lemmatize(words)\n",
        "            lis.append(words)\n",
        "    dataset[\"cleaned_review\"][i] = \" \".join(lis)\n",
        "\n",
        "#     j += 1\n",
        "#     if j > 0:\n",
        "#         break\n",
        "print(dataset[\"cleaned_review\"][69])\n",
        "data_pos = dataset[dataset[\"label\"]==1][\"cleaned_review\"]\n",
        "data_neg = dataset[dataset[\"label\"]==0][\"cleaned_review\"]\n",
        "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
        "              collocations=False).generate(\" \".join(data_pos))\n",
        "plt.figure(figsize = (20,20))\n",
        "plt.imshow(wc)\n",
        "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
        "               collocations=False).generate(\" \".join(data_neg))\n",
        "plt.figure(figsize = (20,20))\n",
        "plt.imshow(wc)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_data, y_data = np.array(dataset['cleaned_review']), np.array(dataset['label'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n",
        "                                                    test_size = 0.05, random_state = 0)\n",
        "print('Data Split done.')\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "Embedding_dimensions = 100\n",
        "\n",
        "# Creating Word2Vec training dataset.\n",
        "Word2vec_train_data = list(map(lambda x: x.split(), X_train))\n",
        "# Defining the model and training it.\n",
        "word2vec_model = Word2Vec(Word2vec_train_data,\n",
        "                 vector_size=Embedding_dimensions,\n",
        "                 workers=8,\n",
        "                 min_count=5)\n",
        "\n",
        "print(\"Vocabulary Length:\", len(word2vec_model.wv.key_to_index))\n",
        "input_length = 750\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "vocab_length = 35000\n",
        "\n",
        "tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n",
        "tokenizer.fit_on_texts(X_data)\n",
        "tokenizer.num_words = vocab_length\n",
        "print(\"Tokenizer vocab length:\", vocab_length)\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\n",
        "X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"X_test.shape :\", X_test.shape)\n",
        "embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n",
        "\n",
        "for word, token in tokenizer.word_index.items():\n",
        "    if word2vec_model.wv.__contains__(word):\n",
        "        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n",
        "\n",
        "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)\n",
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import SimpleRNN,Dense,Activation,Bidirectional,GlobalMaxPool1D\n",
        "from keras.utils.vis_utils import plot_model\n",
        "def getModel():\n",
        "    embedding_layer = Embedding(input_dim = vocab_length,\n",
        "                                output_dim = Embedding_dimensions,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=input_length,\n",
        "                                trainable=False)\n",
        "\n",
        "    model = Sequential([\n",
        "        embedding_layer,\n",
        "        SimpleRNN(100,input_shape = (vocab_length,input_length),return_sequences=False,activation=\"LeakyReLU\"),\n",
        "        Dense(32,activation=\"relu\"),\n",
        "        Dense(1,activation=\"sigmoid\"),\n",
        "    ],\n",
        "     name=\"Sentiment_Model\")\n",
        "    return model\n",
        "training_model = getModel()\n",
        "training_model.summary()\n",
        "plot_model(training_model, \"RNN.png\", show_shapes=True)\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
        "             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]\n",
        "training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = training_model.fit(\n",
        "    X_train, y_train,\n",
        "    batch_size=1024,\n",
        "    epochs=20,\n",
        "    validation_split=0.1,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "Hr19Sdw1j_mc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}